---
title: "Linear Models and Mixed Models"
subtitle: "Wasser Cluster Lunz, Lunz am See"
author: "Aurélien Boyé"
date: "08/10/2019"
output:
  xaringan::moon_reader:
    css: ["default", "theme.css", "theme-fonts.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      countIncrementalSlides: true
      highlithLines: true
editor_options: 
  chunk_output_type: console
---

```{r setup, echo = F}
knitr::opts_chunk$set(
  comment = "#",
  collapse = TRUE,
  #cache = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width=6, fig.height=6,
  fig.align = 'center'
)
```

```{r, include = FALSE}
if (!require(knitr)) install.packages("knitr")
library(knitr)

if (!require(dplyr)) install.packages("dplyr")
library(dplyr)

if (!require(gvlma)) install.packages("gvlma")
library(gvlma)
```

# A brief reminder

.large[
$$Y_{i} = \beta_{0} + \beta_{1} x_{i1} + \cdots + \beta_{p} x_{ip} + \epsilon_{i}$$
]

.pull-left[
- $Y_i$ is the response variable
- $β_0$ is the intercept of the regression line
- $β_p$ is the coefficient of variation for the *nth* explanatory variable
- $x_{ip}$ is the *nth* explanatory variable
- $ε_i$ are the residuals of the model
]

.pull-right[


```{r, echo=FALSE, out.width=450}

knitr::include_graphics("images/linear_regression_1.png")

```

]

---

# Assumptions


.large[
$$Y_{i} = \beta_{0} + \beta_{1} x_{i1} + \cdots + \beta_{p} x_{ip} + \epsilon_{i}$$
]


.pull-left[

1. The residuals are independent
2. The residuals are normally distributed
3. The residuals have a mean of 0
4. The residuals are homoskedastic (they have constant variance)

]

.pull-right[

```{r, echo=FALSE, out.width=400}

knitr::include_graphics("images/linear_regression_2.png")

```

]

--

.center[

.comment[These assumptions concern the residuals, not the response or explanatory variables]

]

---

# Multiple linear regression

**Generalization of simple linear regression**

.center[
.large[
$$Y_i = \alpha + \beta_1x_{1i}+\beta_2x_{2i}+\beta_3x_{3i}+...+\beta_kx_{ki} + \epsilon$$
]
]

&nbsp;

- **Response variable** $\Rightarrow$ 1 continuous variable
- **Explanatory variables** $\Rightarrow$ 2 or more continuous variables

&nbsp;

**Assumptions** (.small[*In addition to the usual assumptions of linear models*])
- Linear relationship between each explanatory variable and response variable
- Explanatory variables are orthogonal (.alert[No collinearity])
  - Collinearity produces unstable parameters estimate and large $se$ for the linear terms

---

# Workflow

.small[

1. **Plot the data**
2. **Create the model** $\Rightarrow$ select your variables
    - Hypotheses (which predictors? form of the relationships?)
    - Collinearity among predictors
    - Stepwise regression analysis / Ranking models
3. **Test the model assumptions** 
    - Check the residuals
4. **Adjust the model if assumptions are violated**
    - Transform response variable
    - Use other tools (e.g. Generalised Linear Models)
5. **Interpret the results**
    - $R^2$ and significance
    - Regression parameters and effect size estimates
    - Visualize the model

]

---

# Workflow


```{r, echo=FALSE, out.width=600}
knitr::include_graphics("images/Zuur_2010.png")
```

&nbsp;

```{r, echo=FALSE, out.width=600}
knitr::include_graphics("images/Zuur_2016.png")
```


.footnote[Accessible [here](http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2009.00001.x/full) and [here](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12577) (with R code in the supplementary material!)]

---

class: inverse, center, middle

background-image: url("images/r_transition.png")
background-size: cover

---

# The iris dataset

```{r}
# Load the data
data(iris)
```

```{r, echo = FALSE}
kable(head(iris,3), format = "html") 
```

```{r}
summary(iris, digits = 2)
```

---

# 1. Plot the data

.pull-left[

```{r iris-plot, eval=FALSE}
# A glimpse of the data
library(ggplot2)

ggplot(data = iris, 
    mapping = aes(x = Petal.Length, 
                  y = Petal.Width, 
                  color = Species)) + 
  geom_point()
```

```{r, echo=FALSE, out.width=300}

knitr::include_graphics("images/iris_versicolor.jpg")

```

]

.pull-right[

&nbsp;

&nbsp;

```{r iris-plot-out, ref.label="iris-plot", echo=FALSE}
```
]

---

# 2. Create the model : collinearity ?

```{r}
# Correlation among the potential predictors of Petal Width
var_cor <- cor(iris[,1:3], method = "pearson", use="pairwise.complete.obs")

var_cor
```

.pull-left[

```{r corplot, eval=FALSE}
# Graphical representation
library(ggcorrplot)

ggcorrplot(var_cor, 
  hc.order = TRUE, 
  type = "upper", 
  lab = FALSE, 
  method="circle", 
  colors = c("tomato2", "white", 
             "springgreen3"))

```
]

.pull-right[
```{r corplot-out, ref.label="corplot", echo=FALSE, out.width = '300px'}
```
]


---

# 2. Create the model : collinearity ?

- **Variance Inflation Factors** (VIF) quantify the extent of correlation between each predictor and the other predictors in the model.

$$VIF = \frac{1}{1-R^2}$$

- Variables with .alert[VIF > 5] are usually considered collinear.

- Can be computed using the `vif()` function of package `car` or using functions from the `usdm` (.small[*Uncertainty Analysis for Species Distribution Models*]) package : `vif()` or `vifcor()` or `vifstep()` and `exclude()`

```{r}
# Collinearity among the potential predictors of Petal Width
usdm::vif(iris[,1:3])
```

---

# 2. Starting simple

```{r, eval = TRUE}
mod <- lm(Petal.Width ~ Petal.Length, data = iris)
```

.pull-left[

&nbsp;

&nbsp;

```{r residuals, eval = FALSE}
# Diagnostic of the residuals
par(mfrow=c(2,2))
plot(mod)
```
]

.pull-right[
```{r, ref.label="residuals", echo=FALSE}
```
]



---

# Residuals vs fitted

This plot shows if residuals have non-linear patterns. If you find equally spread residuals around a horizontal line without distinct patterns, that is a good indication you don’t have non-linear relationships.

&nbsp;

```{r, echo=FALSE, out.width=700}

knitr::include_graphics("images/residuals_1.jpg")

```

---

# Normal Q-Q

Are residuals normally distributed ? It’s good if residuals are lined well on the straight dashed line.

&nbsp;

```{r, echo=FALSE, out.width=700}

knitr::include_graphics("images/residuals_2.jpg")

```

---

# Scale-Location

Also called Spread-Location plot. This plot shows if residuals are spread equally along the ranges of predictors. This is how you can check the assumption of equal variance (homoscedasticity). It’s good if you see a horizontal line with equally (randomly) spread points.

&nbsp;

```{r, echo=FALSE, out.width=700}

knitr::include_graphics("images/residuals_3.jpg")

```

---

# Residuals vs Leverage

Influential outliers ? We watch out for outlying values at the upper right corner or at the lower right corner. Look for cases outside of a dashed line, Cook’s distance.

```{r, echo=FALSE, out.width=700}

knitr::include_graphics("images/residuals_4.jpg")

```

--

.alert[You should never remove outliers if you don't have good reasons to do so (ex: error of measurement) and if you do so, say it !]

---

# Back to our model

```{r, echo = FALSE, out.width = 500}
par(mfrow=c(2,2))
plot(mod)
```

--

.comment[**NOTE**: The `gvlma()` function from `gvlma` package offers a way to automatically check all the major assumptions of linear models]

---

```{r}
gvlma::gvlma(mod)
```

---

# May be not so bad in that case

.pull-left[

```{r iris-plot-lm, eval=FALSE}
ggplot(data = iris, 
    mapping = aes(x = Petal.Length, 
                  y = Petal.Width)) + 
  geom_point() +
  geom_smooth(method ="lm")
```

```{r, echo = FALSE}
plot(mod, which = 3)
```


]

.pull-right[

&nbsp;

&nbsp;

```{r iris-plot-lm-out, ref.label="iris-plot-lm", echo=FALSE}
```
]

---

# May be not so bad in that case

```{r}
d <- iris
d$predicted <- predict(mod) # Save the predicted values
d$residuals <- residuals(mod) # Save the residual values
```


.pull-left[

```{r iris-plot-lm2, eval=FALSE}
ggplot(data = d, 
   mapping = aes(x = Petal.Length, 
                 y = Petal.Width)) +
 geom_smooth(method = "lm", 
             se = FALSE, 
             color = "lightgrey") + 
 geom_segment(aes(xend = Petal.Length, 
                  yend = predicted), 
              alpha = .2) +
 geom_point(aes(color = abs(residuals), 
                size = abs(residuals))) + 
 scale_color_continuous(low = "blue", 
                        high = "red") +
 guides(color = FALSE, size = FALSE) + 
 geom_point(aes(y = predicted), shape = 1) +
 theme_linedraw()
```

]

.pull-right[

&nbsp;

```{r iris-plot-lm-out2, ref.label="iris-plot-lm2", echo=FALSE}
```
]

---

# 5. Interpretation of the outputs

```{r}
summary(mod)
```


---

# 5. Interpretation of the outputs

.left-column[


**Estimates**

&nbsp;

&nbsp;

&nbsp;

&nbsp;

**Standard Error**

&nbsp;

&nbsp;

**t-value**
]

.right-column[

- For the intercept: expected value when all the features are at 0 $\Rightarrow$ meaning ? (.comment[centering predictors]) 

- For the predictors: expected change in the response due to a unit change in the predictor (.comment[standardizing predictors]) 

&nbsp;

- The standard error of the estimates $\Rightarrow$ allow to construct marginal confidence intervals for the estimates, which is given by $\hat{\beta}_i\pm 2\cdot se(\hat{\beta}_i)$ for the 95% confidence interval

&nbsp;

- $t = \frac{\hat{\beta}_i - 0}{se(\hat{\beta}_i)}$ $\Rightarrow$ tells us about how far our estimated parameter is from a hypothesized 0 value

]

---

# 5. Interpretation of the outputs

```{r}
mod$coef

summary(mod)$coef

confint(mod)

summary(mod)$r.squared

summary(mod)$adj.r.squared
```

---

# 5. Interpretation of the outputs

- **Centering the predictors**

The slopes between the predictors and the response variable doesn’t change but .alert[the interpretation of the intercept does].

--

&nbsp;

.pull-left[
**Before centering**

.small[
Intercept = mean of the response when all predictors == 0
]
]

.pull-right[
**After centering**

.small[
Intercept = mean of the response when all predictors are at their mean
]
]

&nbsp;

.comment[

1. Do we want to interpret the intercept ?

2. Is 0 meaningful ?

2. Is the mean meaningful ? $\Rightarrow$ center on other values ?

]

---

# 5. Interpretation of the outputs

- **Standardizing the predictors**

The coefficients estimated are now standardized slopes, meaning that the units of the regression coefficients are the same 

&nbsp;

--

.pull-left[
**Before standardizing**

.small[
Slopes = expected change in the response due to a unit change in the predictor
]
]

.pull-right[
**After standardizing**

.small[
Slopes = expected change in the response due to a change of one **standard deviation** in the predictor
]
]

--

.comment[
.small[
1. Standardized coefficients are comparable within models and between studies $\Rightarrow$ standardized effect size estimates (.alert[but beware of the range covered for of each variable])

2. Unstandardized effect size estimates may be more meaningful in some context because they depend on the phenotypic variation in each study population
]
]

---

# Centered vs standardized predictors

```{r}

# Initial model
mod <- lm(Petal.Width ~ Petal.Length + Sepal.Width, data = iris)

# Model with centered variables
iris_cent <- iris
iris_cent[,c("Petal.Length","Sepal.Width")] <-
  scale(iris_cent[,c("Petal.Length","Sepal.Width")], 
        center = TRUE, scale = FALSE) 

mod_cent <- lm(Petal.Width ~ Petal.Length + Sepal.Width, data = iris_cent)

# Model with standardized variables
iris_stand <- iris
iris_stand[,c("Petal.Length","Sepal.Width")] <-
  scale(iris_stand[,c("Petal.Length","Sepal.Width")], 
        center = TRUE, scale = TRUE) 

mod_stand <- lm(Petal.Width ~ Petal.Length + Sepal.Width, data = iris_stand)

```

---

# Centered vs standardized predictors

```{r echo = TRUE, width = 700}
par(mfrow=c(2,2))
plot(mod)
```

---

# Centered vs standardized predictors

```{r echo = TRUE, width = 700}
par(mfrow=c(2,2))
plot(mod_cent)
```

---

# Centered vs standardized predictors

```{r echo = TRUE, width = 700}
par(mfrow=c(2,2))
plot(mod_stand)
```

---

# Centered vs standardized predictors

```{r}

summary(mod)$coef

summary(mod_cent)$coef

summary(mod_stand)$coef
```


---

# Centered vs standardized predictors

.pull-left[
```{r, echo=FALSE, out.width=700}
knitr::include_graphics("images/coef_raw.png")
```


]

.pull-right[
```{r, echo=FALSE, out.width=700}
knitr::include_graphics("images/coef_cent_intercept.png")
```


]

.footnote[
See the function `ggcoef()`from the `GGally` package
]

---

# Centered vs standardized predictors

.pull-left[
```{r, echo=FALSE, out.width=700}
knitr::include_graphics("images/coef_raw.png")
```

```{r, echo=FALSE, out.width=700}
knitr::include_graphics("images/coef_cent.png")
```
]

.pull-right[
```{r, echo=FALSE, out.width=700}
knitr::include_graphics("images/coef_cent_intercept.png")
```

```{r, echo=FALSE, out.width=700}
knitr::include_graphics("images/coef_stand.png")
```
]

.footnote[
See the function `ggcoef()`from the `GGally` package
]

---

# Centered vs standardized predictors

&nbsp;

- This applies to Linear Mixed Models (LMM), Generalized Linear Models (GLM) and  Generalized Linear Mixed Models (GLMM)

--

&nbsp;

&nbsp;

```{r, echo=FALSE, out.width=600}

knitr::include_graphics("images/Schielzeth_2010.png")

```

.footnote[Accessible [here](https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/j.2041-210X.2010.00012.x)]


---

class: inverse, center, middle

# Reality is often more complex

## *Structures in the data*

---

# A more complicated example

.pull-left[

&nbsp;

```{r iris-plot-sepal-1, eval=FALSE}
# A glimpse of the data
library(ggplot2)

ggplot(data = iris, 
    mapping = aes(x = Sepal.Length, 
                  y = Sepal.Width, 
                  color = Species)) + 
  geom_point()
```

]

.pull-right[

&nbsp;

&nbsp;

```{r iris-plot-sepal-1-out, ref.label="iris-plot-sepal-1", echo=FALSE}
```
]

---

# A more complicated example

.pull-left[

&nbsp;

```{r iris-plot-sepal-2, eval=FALSE}
# A glimpse of the data
library(ggplot2)

ggplot(data = iris, 
    mapping = aes(x = Sepal.Length, 
                  y = Sepal.Width, 
                  color = Species)) + 
  geom_point() +
  geom_smooth(aes(x = Sepal.Length, 
                  y = Sepal.Width), 
              inherit.aes = FALSE, 
              method = "lm", 
              col = "black")
```

]

.pull-right[

&nbsp;

&nbsp;

```{r iris-plot-sepal-2-out, ref.label="iris-plot-sepal-2", echo=FALSE}
```
]

---

# A more complicated example

.pull-left[

&nbsp;

```{r iris-plot-sepal, eval=FALSE}
# A glimpse of the data
library(ggplot2)

ggplot(data = iris, 
    mapping = aes(x = Sepal.Length, 
                  y = Sepal.Width, 
                  color = Species)) + 
  geom_point() +
  geom_smooth(aes(fill = Species), 
              method = "lm")
```

]

.pull-right[

&nbsp;

&nbsp;

```{r iris-plot-sepal-out, ref.label="iris-plot-sepal", echo=FALSE}
```
]

---

```{r}
lm(Sepal.Width ~ Sepal.Length*Species, data=iris)$coef


lm(Sepal.Width ~ Sepal.Length*Species - 1, data=iris)$coef

lm(Sepal.Width ~ Sepal.Length + Species + Sepal.Length:Species - 1, data=iris)$coef
```

---

class: inverse, center, middle

# Optional section

## *Non-linear relationships ? *
---

## Transformation of variables

## Polynomial regressions

## GLM

---

# Variance partitioning

---

# Session info

```{r session_info, out.width = "50%"}

sessionInfo()

```
